[{"content":" Data are like humans. You have to spend time with them to understand them.\n ","href":"/","title":"Home"},{"content":"","href":"/post/","title":"Posts"},{"content":" Here is a paragraph. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\nHeading 2 Another one. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\nHeading 3 Yet another, but centered! Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n Heading 4  First item Second item  Nested unordered item  Third item  Nested ordered item 1 Nested ordered item 2   Heading 5 Where are the quotes!!!\n Simplify, then add lightness.\n— Colin Chapman\n Now, time for some links!\n GoHugo  Hugo Themes   Heading 6 Inline code: echo \u0026quot;What is the meaning of life?\u0026quot;. Who knows?\n// Codeblock  var meaningOfLife = 42; console.log(\u0026#39;The meaning of life is: \u0026#39;, meaningOfLife);  Who wants some table?\n   Minimo Caption More Caption     Cool What? Now, wut?!    Ah, enough for today, eh?\n","href":"/typography/","title":"Typography"},{"content":"This site, deepLearner, is not about Deep Learning as we understand in the field of Data Science. Rather, the purpose here is to express deep learning in the sense of learning something at the core. In other words, learning and taking it from shallow to a deeper level of understanding.\nWe are all learners. Some of us are deep learner. That’s deepLearnR.\nOf course learning the technologies, i.e., data science, in particular, will be a major focus of our articles. Most importantly, statistical thinking will be at the core.\nIt is worth mentioning that many aspects of data science overlap with what has been there for over hundred years. Statisticians played and continue to play a central role in knowledge discovery from data. However, a new era has emerged with a massive amount of data from almost every aspect of our lives. Thus, it has opened many new possibilities where the use of deep learning techniques has become essential.\nIn short, problems that statisticians solve are, by and large, different than the problems that artificial intelligence community solves.\nBoth have their places and relevance in the present time.\n","href":"/about/","title":"About"},{"content":" Github needs no introduction. If you are unfamiliar with git, its a version control system for tracking changes in computer (programming) codes. \u0026ldquo;GitHub Inc. is a web-based hosting service for version control using Git. It is mostly used for computer code. It offers all of the distributed version control and source code management functionality of Git as well as adding its own features.\u0026rdquo;1\nCopying a remote repository to local To bring a remote repository to your local host (computer where you are working), use the git clone command.\ngit clone https://github.com/username/repository_name.git Publishing the changes After you update your code or add or remove anything from the repository, you want to publish to the remote repository.\nPublishing the changes is called push in git jargon. For that you need to write the following codes.\ngit add -A git commit -m \u0026#34;Message/notes of what the changes are\u0026#34; This will update the local repository. Now you want to publish the changes using the following command\ngit push You may want to be more specific about which remote to update. The default remote is origin. The command would be\ngit push origin master Syncing a remote repository on two local hosts Sometime I work on two or three different computers for the same remote project. So keeyping the repositories in sync is essential. I want my codes to be synced accross multiple computers.\nFor that I follow these simple steps.\n- Pull the remote repository in your new workstation - Make changes to your code - Push (publish) the updates  The key is to pull and update the local repository assuming the remote has the latest updates.\ngit pull git add -A git comit -m \u0026#34;Update message\u0026#34; git push origin master Last updated: 2018-07-30\n Wikipedia\r[return]   ","href":"/post/frequently-used-git-commands/","title":"Frequently used git commands"},{"content":"Microsoft nicely integrates R with its Power BI desktop. This is a test.\n","href":"/post/integrating-r-with-powerbi/","title":"Integrating R with PowerBI"},{"content":"Today is 7/26\nThis is test post to check if Python code works\nlibrary(reticulate)\rimport numpy as np\rx = [1, 2, 3]\rprint(x)\r## [1, 2, 3]\ry = np.mean(x)\r\\(\\bar{x}\\) is the average, which is defined as \\[\r\\bar{x} = \\frac{\\sum x}{n}\r\\]\nSo what is the aveage of y? It is 2.\nx = c(1:5)\rThe value of x is R session is 1, 2, 3, 4, 5.\nMore text is to be added.\nR Markdown\rThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars)\r## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00\rfit \u0026lt;- lm(dist ~ speed, data = cars)\rfit\r## ## Call:\r## lm(formula = dist ~ speed, data = cars)\r## ## Coefficients:\r## (Intercept) speed ## -17.579 3.932\r```\n\rTesting LaTeX\rThis is an inline test for math content in markdown \\(\\bar{x}\\). And this is a mathe block, defining the average– \\[\r\\bar{x} = \\frac{\\sum x}{n}\r\\]\nAnd some more complicated formula\n\\[\ry = x_1 \\beta_1 + x_2 \\beta_2 + \\epsilon\r\\] is the multiple linear regression model with two covariates \\(x_1\\) and \\(x_2\\).\nWhen \\\\( a \\ne 0 \\\\), there are two solutions to\r$$ ax^2 + bx + c = 0 $$\rand they are:\r\\\\[ x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a} \\\\]\r\r","href":"/post/python-using-rstudio-ide/","title":"Python using RStudio IDE"},{"content":" A lot of times interested folks contact me with a question like this: ‘I want to be a data scientist; can you help?’\nWell, I wish I could. The reality is, I won’t be able to unless I really understand what you want and why you want it.\nFirstly because guiding someone is next to impossible in this time and era when everything is so convoluted. People come to learn with preconceived ideas and notions. I am not saying you have to come with a blank slate, what I am saying is you need to show real motivation. I repeat, you must have a motivation and you must be able to articulate why you are motivated.\nThere is no shortage of tutorials, courses, documents, websites, articles today on how to become a data scientist. Sadly, most of the times there is a business mind lurking behind such documents and campaigns. They will tell you only partial truth. They will tell you that you can become a data scientist in 4 months, or in 6 months, or by completing a boot camp and so on and so forth.\nIn my opinion, if you already have the background of a typical data scientist (have worked with data, have knowledge of scientific process, programming skills in R, Python, SAS, or any relevant language), then yes, you can acquire the knowledge and become familiar with the tools that data science utilizes within a short period of time.\nOn the other hand, if you are a complete novice, you will not only be deceived by these campaigns but also become frustrated quickly. This is because you were drawn to something for which you were not prepared for. And you did not anticipate the challenges you would face to become a data scientist.\nNow let’s come to the main point.\nWhy do you want to become one Ask yourself the “why” question and give an honest answer. What is your response?\nIt is perfectly okay to say that you want to be a data scientist because the market is hot.\nBut the best approach would be to write a few points on a piece of paper as to why do you want to become a data scientist.\n Point 1 Point 2 Point 3 Point 4  You could list that you are passionate about working with data; you want to make an impact by helping the management or team leaders making the right decision based on data, or you just enjoy solving other’s problems with data.\nWhatever may be your reasons, it is important to list them down. Listening makes you look at it by yourself. That helps you to assess whether this is really something you want to do.\nMotivation is the key.\nKnow what is data science and who are data scientists Before you jump into this field, do some research about the field. Google it, find some blogs to follow, read some whitepapers from reputed analytic organizations and software companies such as SAS, IBM, Google, Microsoft and many others. Spend some time reading. Not just skimming through; actual reading. To me, reading is the best way to broaden knowledge. In this era of YouTube and video lectures, I still find reading somewhat superior, especially for beginners. Once you have acquired a general understanding of the field, listening to lectures or watching short video clips would boost your knowledge quickly.\nWhat do data scientists do I will write briefly. I am working work on a separate article on it.\n Data scientists solve problems with the help of data. That’s what they do.\n On paper, Data Scientists do so many things that someone with a title of data scientist in one industry cannot possibly list what another data scientist might be doing in a different industry. That is to give you an idea of how variable the type of works data scientists is involved with.\nSimply put, data scientists work with data to bring actionable insight that helps to solve a problem. If the data scientist is working in a financial organization, the problems would be related to, say, banking industry, the stock market, etc. If the data scientist works in healthcare, the problem would be related to healthcare service provider such as hospitals, health units.\nData scientists do not necessarily get involved with building the data architecture such as data warehousing or setting up or maintaining cloud infrastructure for an organization. Those who work in this type of work are called ‘Data Engineers’. You may want to read a little more about this in my earlier post (in Bengali).\nLet me clarify one thing–data scientists do not necessarily design the databases. It is the data engineers who do that. Data scientists analyze the data to drive business whereas data engineers develop and maintain the architecture that stores data. Data scientists or statisticians do not build or maintain databases in most of the cases. However, there is often a need to create smaller databases for a specific purpose or a project. A data scientist may need to design, develop and maintain project-specific databases, which are largely different from enterprise-level data warehouse.\nMost large organizations have their separate team of engineers who develop and maintain the data-science platforms (DBs, Hadoop, etc.).\nHow to become a data scientist Everyone wants to be a data scientist but only some will be successful. The reason is in part, your preparation, in part your effort. If you are not from a quantitative background with some math, and computer programming experience, I would say it would be a daunting task for you to learn what it takes to be a data scientist. I am completely being honest.\nAt some point in time, you are going to read and write computer programs.\nIf you’ve never written a computer program, I am not sure how long it will take for you to pick up a language. But I personally feel that it takes at least 6 months to get some basics of any programming language. Another year or so to know a bit more. If you engage yourself full time to learn a language, you might be able to write simple programs and carry out simple data science tasks within 4-6 months.\nTwo approaches to be a Data Scientist Learn the tools to become data scientist, or Find the problems you want to solve, and then learn what it takes to solve them\nThe first approach is the typical approach and I call it bottom-up approach. I will not discuss the first approach because it has been the mainstream approach thus far. This has brought so many discussions, lot of training offerings, online courses, articles in the media, blogs, and vlogs, and much more.\nI will focus on the second approach to becoming a data scientist. I call it top-down approach.\nFind problems that you want to solve This is perhaps the easiest way to find out if you really want to be a data scientist. If so, keep reading.\nI call this approach a top-down approach, you first find a problem and then solve it.\nAs I wrote earlier in this article, data scientists solve problems with the help of data. In other words, they have a problem to solve in front of them and they would provide a solution using data.\nNow this may sound like, well, problem is there, the data is there, so what’s the big deal?\nIt’s not a big deal, but its a huge deal.\nHaving the data is only one step to solving the problem. What if you do not have the data in the right form, or you do not have the data in the first place. If you have the data, it often takes 80% of your time to prepare it for your analysis.\nIf you do not have the data, then you first need to get the data in the first place.\nWhere do I find problem? Well, problems are everywhere, you just have to look around.\nI am going to give you some ideas which should give you enough clues to find a problem on your own and discover what it takes to solve those problems. Here goes the list with brief descriptions.\nProblem 1: Building a list of most important news of the day\nThis may sound like an unattractive problem. But think about how you might want to use the information. First, you need to think about a focus area. If you are interested in politics, the possibilities are endless. If you build a list exclusively on “North Korea”, you would find at the end of the year how the things unfolded and you can present the information on a timescale. The importance of such a list is to be able to find temporal relationship between different events which may affect some other event/outcome of interest.\nYou might be aware that news or sometimes a tweet from an influential person can affect the stock price of a particular company in the US stock market. But this could be useful beyond US market. You will find many potential use cases if you think a bit deep.\nProblem 2: Tracking accident statistics\nThis problem is particularly suitable for resource-limited countries where government record keeping is inadequate. Every day newspapers report accidental deaths due to road crashes. You could select one or two major newspapers and scan their pages to retrieve news about road accidents. This could be done as part of a research project at your institution where you pursue a Bachelors or Masters degree.\nProblem 3: Calculating impact of faculty research\nThis is something you can do for your department, or institution or country. Start with your department first where you have only a handful of faculties who are presumably doing some kind of research and publishing their findings in journals. Think of developing an automated system that would find articles (in Google Scholar, for example) and calculate impacts (from the journal impact factors). You can present the results by department, by type of research, or by institution.\nProblem 4: Build an economic performance dashboard\nYou can use Google’s Data Studio to build a dashboard of economic performance of your country on some key performance metrics. The data could be found on the central bank’s website or government data repositories. For Bangladesh, many economic data sets are available online free of charge. Just visit https://www.bb.org.bd/econdata/ and find your data there.\nConclusion In conclusion, you can take the easier route to be a data scientist. In this route, you first need to find a problem and you will point your learning path towards solving that problem. The path is not smooth. But you can always ask questions to someone who knows or someone more experienced. It is a good practice to find someone who would be your mentor and will guide you through your learning path. Since you are learning on your own, you have to take the responsibility on your own. If you do not want to learn, nobody is going to push. If you are really interested and motivated, I hope this article has given you some direction.\nHappy learning.\n","href":"/post/to-the-aspiring-data-scientists/","title":"To the Aspiring Data Scientists"},{"content":" If you are exploring the hot field of Data Science today, you probably know by now that there is no unique way to define data science. Everyone’s perspective comes from their respective backgrounds. Statisticians think they are data scientists while CS folks think they are.\nI would not go into the debate.\nBoth have truth in their arguments as the typical problems they solve are quite different. There are some exceptions where their workflows overlap. Let me provide some thoughts from my perspective.\nI am a statistician by training and I have been in the analytic world for over 15 years. This includes time spent at universities teaching graduate level statistics courses as well as working in the healthcare industry as a data scientist.\nThree types of Statisticians Like many scientific disciplines, statisticians contribute to the science in two different ways.\nFirst, through theoretical contribution to the scientific literature to advance the knowledge. In this type of work, statisticians focus on developing new methods to solve new problems or improving existing methods. In statistical science, any new theoretical development takes several years, sometimes decades, to gain popularity among the mainstream practitioners. Part of it is due to the availability of software packages or lack thereof that the practitioners are most familiar with. And there are some other reasons too.\nSecondly, the works of applied statisticians and applied researchers including epidemiologists who focus on solving problems that need an immediate solution. Most of the time they utilize already developed statistical methods in their workflow. Here’s where most of the overlap happens between the works of applied statisticians and practitioners of statistics.\nHowever, there is no strict boundary between applied and theoretical statisticians. Often, applied statisticians develop new methods to solve an existing problem. Thus, the third kind of statisticians are those who fall somewhere in between theoretical statisticians and applied statisticians.\nTowards Data Science There are, however, some philosophical differences between these two types of statisticians. Theoretical researchers often think they are superior to their counterpart, while the applied folks think they are the ones whose contributions have a real impact. Both are wrong, in my honest opinion.\nCompared to their peers, Applied Statisticians find their views better aligned with the theme of today’s data science. This is perhaps because of the way they were trained during their academic years.\nApplied statisticians are taught and trained to use data to bring insights out of it. They are trained not only to fit a model and know how it works but also to communicate with their customers in non-technical terms. Intuition is all that takes priority over theoretical derivations. As Jo Hardin points 1,\n .. when teaching, for example, the Neyman-Pearson lemma, the intuition behind how we know what we know (and why it matters) is vastly more fundamental for the students’ future research capabilities than the detailed steps of the proof.\n I think it is vital for a student of statistics to learn and think this way to become successful in today’s job market.\nData Scientist Vs Data Engineers The difference between many types of data scientists is often confusing even to the statisticians. Sometimes, data scientists and data engineers are considered equally. In fact, the type of work they do is quite different.\nLet me clarify one thing–data scientists do not necessarily design the databases. It is the data engineers who do that. Data scientists analyze the data to drive business whereas data engineers develop and maintain that architecture. Data scientists or statisticians do not build or maintain databases. Most large organizations have their separate team of engineers who develop and maintain the data-science platforms (DBs, Hadoop, etc.).\nDo the statisticians need to know a bit of database? Yes, a basic understanding is good enough. Most of the time all you will do is pull some tables from different sources and join them to create a working data set. For that, you need to understand basics and you do not have to be a data architect for that. If you are working in a startup company, then you may be required to understand in greater depth, though as they have less manpower.\nHow big of a deal knowing how to efficiently join tables? Not so big. Anyone with a decent knowledge of data and SQL can learn to do it with some reading and practice. But you have to have the mindset of learning in the first place.\nUse of Statistics In my work, I find statistics invaluable although most of the time we do not use many advanced techniques. And this is by far true for most organizations who utilize data for decision-making. High-level analysis such as modeling and machine learning stay at the very top level of the application pyramid. You still need to lay the foundation using basic analysis and visualizations.\nHow media is portraying data science today (such as deep learning and AI) is what perhaps 1% of all the analytics an organization needs. Many large organizations do not hire people for that. They purchase a solution instead as that is more cost-effective. For many problems where deep learning is not applicable, they need analytic people with decent statistical literacy.\nConclusion Data Science as it’s being understood today solves problems that are, for the most part, quite different than the problems that would need statisticians’ help to solve. Being on both sides of the aisle I can see how they complement each other and how they both are relevant.\nLet me know what’s your thoughts are. And please share this article if you find it useful.\nThank you!\n Jo Harding (2017). Expectations and Skills for Undergraduate Students Doing Research in Statistics and Data Science, AMSTAT Newsletter\r[return]   ","href":"/post/statisticians-in-the-data-science-era/","title":"Statisticians in the Data Science Era"},{"content":"I would like to share a few things with you from recent experience of my transition from academia to industry. Of course I have a lot more experience than a fresh graduate, but something I’ve experienced that many others may benefit from. I believe this will help you to think at least. I’ve written this with the candidates in the US job market in mind.\nFirstly, I would say this– lies, damn lies, statistics, AND one-page resume. I hope you got what I meant. One-page resume never worked for me. Here’s why.\nHiring managers rarely review your resume when you apply for a job. It normally goes through an initial round of review by recruiters. Organizations have partnered with recruiting companies and they do these initial screening. Literally, they will never forward any resume that is one-page. Because one-page resume rarely tells what you have accomplished. It’ll only make you look like hundred or thousand others candidates who have the exact same background and experience (R, SAS, courses, etc.) as you have.\nYou have to literally describe each of your projects and your contributions. You don’t have to describe every details. In fact you should not reveal every details but sufficient enough so that the recruiter understands it and convinced enough to forward your resume.\nThe hiring manager would then scan only your first page, perhaps. So prepare your resume as such.\nSecondly, follow the jobs market closely and regularly. See what the market is looking for. That will put you into the real perspective because as a result of you being well informed, your resume will reflect that.\nThirdly, you need to submit your well detailed resume to several job sites such as CareerBuilder, Indeed, LinkedIn, Monster, Dice etc. I found CareerBuilder and Indeed to be better for stat (industry) jobs. LinkedIn is also good for finding local opportunities.\nPLEASE spend a lot of time to properly submit your resume to these job sites with all possible keywords appropriate for your discipline. It will take about 2 to 3 weeks before the resume is propagated to various recruiters’ networks. Once recruiters find your resume, they will directly call you. Recruiters get a good amount of commission when they are able to forward a good candidate and the company hires the candidate. So they always try to help you. Often they will suggest you to modify your resume to make it suitable for that particular position.\nFourth, have patience. A lot of patience. You have to knock every possible door, and only some doors will open.\nFifth, prepare yourself for the interview. I find Liz Ryan (www.linkedin.com/in/lizryan) to be excellent. You will find her in LinkedIn. Follow her. It’s really really helpful. I wish I would have found her earlier.\n","href":"/post/preparing-for-statistics-jobs-in-the-industry/","title":"Preparing for statistics jobs in the industry"},{"content":"R Markdown\rThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars)\r## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00\rfit \u0026lt;- lm(dist ~ speed, data = cars)\rfit\r## ## Call:\r## lm(formula = dist ~ speed, data = cars)\r## ## Coefficients:\r## (Intercept) speed ## -17.579 3.932\r\rIncluding Plots\rYou can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1))\rpie(\rc(280, 60, 20),\rc(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;),\rcol = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;),\rinit.angle = -50, border = NA\r)\r\rFigure 1: A fancy pie chart.\r\r\r","href":"/post/2015-07-23-r-rmarkdown/","title":"Hello R Markdown"},{"content":" I was asked if we can do the test of equality of two population variances using Stata. Well, I did not have to do it myself ever, but yes, it is possible to do it. Here is an example. I’ve just made-up the data to show the procedures.\nSuppose we have the following data where the variables may represent two independent samples taken from normally distributed populations. And we want to test if the population variances of the two populations are the same. We will use the sample variances to do the testing.\n/* A fictitious data */ input x y 1 2 2 3 2 4 3 5 3 4 4 6 end; To test if the variances are the same, run the following\n. sdtest x = y Variance ratio test ------------------------------------------------------------------------------ Variable | Obs Mean Std. Err. Std. Dev. [95% Conf. Interval] ---------+-------------------------------------------------------------------- x | 6 2.5 .4281744 1.048809 1.399343 3.600657 y | 6 4 .5773503 1.414214 2.515874 5.484126 ---------+-------------------------------------------------------------------- combined | 12 3.25 .4105613 1.422226 2.346361 4.153639 ------------------------------------------------------------------------------ ratio = sd(x) / sd(y) f = 0.5500 Ho: ratio = 1 degrees of freedom = 5, 5 Ha: ratio \u0026lt; 1 Ha: ratio != 1 Ha: ratio \u0026gt; 1 Pr(F \u0026lt; f) = 0.2638 2*Pr(F \u0026lt; f) = 0.5276 Pr(F \u0026gt; f) = 0.7362 There is another way to do this in case if the mean, standard wwwiation and the sample size are available\n/* first, extract the mean and standard wwwiations of the variables */ . summarize // to extract the mean and standard wwwiations of the variables Variable | Obs Mean Std. Dev. Min Max -------------+-------------------------------------------------------- x | 6 2.5 1.048809 1 4 y | 6 4 1.414214 2 6 . sdtesti 6 2.5 1.04 6 4 1.41 Variance ratio test ------------------------------------------------------------------------------ | Obs Mean Std. Err. Std. Dev. [95% Conf. Interval] ---------+-------------------------------------------------------------------- x | 6 2.5 .4245782 1.04 1.408587 3.591413 y | 6 4 .5756301 1.41 2.520296 5.479704 ---------+-------------------------------------------------------------------- combined | 12 3.25 .4091612 1.417376 2.349442 4.150558 ------------------------------------------------------------------------------ ratio = sd(x) / sd(y) f = 0.5440 Ho: ratio = 1 degrees of freedom = 5, 5 Ha: ratio \u0026lt; 1 Ha: ratio != 1 Ha: ratio \u0026gt; 1 Pr(F \u0026lt; f) = 0.2601 2*Pr(F \u0026lt; f) = 0.5202 Pr(F \u0026gt; f) = 0.7399 . Unbalanced Data input grade section 79 1 80 1 65 1 90 1 67 1 77 1 80 1 45 1 86 1 99 2 78 2 36 2 67 2 78 2 81 2 end; . . /* To see how many grades in each section*/ . tab section section | Freq. Percent Cum. ------------+----------------------------------- 1 | 9 60.00 60.00 2 | 6 40.00 100.00 ------------+----------------------------------- Total | 15 100.00 . . /* Doing the variance test */ . sdtest grade, by(section) Variance ratio test ------------------------------------------------------------------------------ Group | Obs Mean Std. Err. Std. Dev. [95% Conf. Interval] ---------+-------------------------------------------------------------------- 1 | 9 74.33333 4.527693 13.58308 63.89246 84.77421 2 | 6 73.16667 8.553427 20.95153 51.17938 95.15395 ---------+-------------------------------------------------------------------- combined | 15 73.86667 4.183717 16.20347 64.89349 82.83985 ------------------------------------------------------------------------------ ratio = sd(1) / sd(2) f = 0.4203 Ho: ratio = 1 degrees of freedom = 8, 5 Ha: ratio \u0026lt; 1 Ha: ratio != 1 Ha: ratio \u0026gt; 1 Pr(F \u0026lt; f) = 0.1322 2*Pr(F \u0026lt; f) = 0.2645 Pr(F \u0026gt; f) = 0.8678 . end of do-file .  Syntax The syntax of the Stata commands can be found in the Stata help file. Run the following commands in your Stata command prompt, and you will see many examples showing the usage of these commands.\nhelp sdtest help sdtesti The syntax for sdtesti for testing the equality of variances for variable1 and variable2 is\n/* for testing variance of two variables */ sdtest variable1 = variable2 /* for testing variance of a single variable */ sdtesti obs . ave stwww // note the dot (.) after obs /* for testing variance of two variables */ sdtesti obs1 ave1 stwww1 obs2 ave2 stwww2 // there is no dot (.) here /* for testing variance of a single variable to some fixed value */ sdtest grade = 2","href":"/post/testing-equality-of-two-population-variances-using-stata/","title":"Testing equality of two population variances using Stata"},{"content":"This is yet another experiment to see how good is the approximation of binomial probability when we use Poisson and normal distributions for scenarios with large n, and p close to zero or one.\nConsider a problem where the random variable \\(X\\) follows a binomial distribution with a known probability of success \\(p\\), and number of trials \\(n\\). If n becomes large, it may not be possible to calculate the probabilities by hand calculation or using a calculator.\nWe can approximate the binomial distribution with a normal distribution or a Poisson distribution.\nAn example\rThe probability that a person will develop an infection even after taking a vaccine that was supposed to prevent the infection is 0.03. In a random sample of 200 people in a community who got the vaccine, what is the probability that six or fewer people will be infected?\nSolution\rLet \\(X\\) denote the number of persons getting infected. Clearly, \\(X\\) follows a binomial probability distribution with \\(n=200\\) and \\(p = 0.03\\). The exact probability of having six or fewer people getting infected is\n\\[\rP(X \\leq 6) = \\sum_{k=0}^{6} \\binom{200}{k} p^k q^{200-k}\r\\]\nThe probability is 0.6063. We can verify the calculation using R as\nsum(dbinom(0:6, 200, .03))\r## [1] 0.6063152\rOr alternatively,\npbinom(6, 200, .03)\r## [1] 0.6063152\rTo avoid such a big calculation by hand, we can approximate the binomial probability using a Poisson distribution or a normal distribution. I will use both and see which one approximates better.\n\r\rPoisson approximation to the binomial distribution\rTo use Poisson approximation to the binomial probabilities, we consider that the random variable \\(X\\) follows a Poisson distribution with rate \\(\\lambda = np = (200)(0.03) = 6.\\) Now, we can calculate the probability of having six or fewer infections as\n\\[\rP(X \u0026lt; 6) = \\sum_{k=0}^{6}\\frac{e^{-6} 6^k}{k!} = 0.6063\r\\] which turns out to be the same as we obtained with the binomial distribution.\nWe can verify the calculation in R\nppois(6, lambda = 6)\r## [1] 0.6063028\rClearly, Poisson approximation is very close to the exact probability.\n\rNormal approximation to the binomial distribution\rWe can also calculate the probability using normal approximation to the binomial probabilities. Since binomial distribution is for a discrete random variable and normal distribution is for continuous random variable, we have to make continuity correction to approximate a binomial distribution using the normal distribution.\nFor large \\(n\\) and when \\(np \u0026gt; 5\\) and \\(nq \u0026gt; 5,\\) a binomial random variable \\(X\\) with \\(X \\sim Bin(n, p)\\) can be approximated by a normal distribution with mean = \\(np\\) and variance = \\(npq.\\) That is, \\(X \\sim N(6, \\, 5.82).\\)\nTherefore, the probability that there will be six or fewer cases of incidences can be calculated as\n\\[\rP(X \\leq 6) = P\\left(z \\leq \\frac{X-6}{\\sqrt{5.82}} \\right).\r\\] As mentioned earlier, we have to make the continuity correction, and so the above expression will become \\[\r\\begin{align*}\rP(X \\leq 6) \u0026amp;= P\\left(z \\leq \\frac{(X+0.5) - 6}{\\sqrt{5.82}} \\right) \\\\\r\u0026amp;= P\\left(z \\leq \\frac{6.5-6}{\\sqrt{5.82}}\\right) \\\\\r\u0026amp;= P(z \\leq 0.2072)\r\\end{align*}\r\\]\nUsing a standard normal table or using R, we can obtain the probability, which is 0.5821\npnorm(.2072)\r## [1] 0.5820732\rWe note that the approximation is close to the exact probability 0.6063 but the Poisson approximation does much better.\n\rSimulation\rTo see how the good the approximations are in repeated samples, we generate 1000 random sample of size 200 from a normal distribution with mean = 6 and standard wwwiation = 5.82. The generated data are used to approximate the binomial probability using Poison and normal distributions.\nWe use the following code to generate the figure below.\napprx \u0026lt;- function(n, p, R = 1000, k = 6) {\rq = 1- p\rtrueval \u0026lt;- pbinom(k, n, p) # true binomial probability\rprob.zcc \u0026lt;- prob.zncc \u0026lt;- prob.pois \u0026lt;- NULL for (i in 1:R) {\rx \u0026lt;- rnorm(n, n * p, sqrt(n * p * q))\rz.cc \u0026lt;- ((k + .5) - mean(x))/sd(x) # with cont. correction\rprob.zcc[i] \u0026lt;- pnorm(z.cc)\rz.ncc \u0026lt;- (k - mean(x))/sd(x) # no cont. correction\rprob.zncc[i] \u0026lt;- pnorm(z.ncc) y \u0026lt;- rpois(n, n * p)\rprob.pois[i] \u0026lt;- length(y[y \u0026lt;= k])/n\r}\rlist(prob.zcc = prob.zcc, prob.zncc = prob.zncc, prob.pois = prob.pois, trueval = trueval)\r}\rR \u0026lt;- 1000\rset.seed(10)\rout \u0026lt;- apprx(n = 200, p = .03, k = 6, R = 1000)\r# windows(6,5)\rplot(1:R, out$prob.pois, type = \u0026quot;l\u0026quot;, col = \u0026quot;green\u0026quot;, xlab = \u0026quot;Runs\u0026quot;, main = expression(paste(\u0026quot;Simulated Probabilities: \u0026quot;, n==200, \u0026quot;, \u0026quot;, p==0.03, sep=\u0026quot;\u0026quot;)),\rylab = \u0026quot;Probability\u0026quot;, ylim = c(.3, .7))\rabline(h = out$trueval, col=\u0026quot;red\u0026quot;, lty=2)\rlines(1:R, out$prob.zcc, lty = 1, col = \u0026quot;purple\u0026quot;)\rlines(1:R, out$prob.zncc, lty = 1, col = \u0026quot;orange\u0026quot;)\rlegend(\u0026quot;bottomleft\u0026quot;, c(\u0026quot;Poisson\u0026quot;, \u0026quot;Normal (with cc)\u0026quot;, \u0026quot;Normal (w/o cc)\u0026quot;),\rlty = c(1), col = c(\u0026quot;green\u0026quot;, \u0026quot;purple\u0026quot;, \u0026quot;orange\u0026quot;))\rAbove figure shows the calculated probabilities for each run of the simulation. The read horizontal line at 0.6 shows the exact probability. The figure shows that the normal approximation, with or without continuity correction, is underestimating the exact probability while Poisson does a better job approximating it for \\(n=200\\) and \\(p=0.03.\\)\nSince this plot does not reveal much due to overlapping points, we draw boxplots for the calculated probabilities for \\(n= 100, 200, 233, 300,\\) and \\(p=0.03.\\)\n# n = 200\rset.seed(10)\rout \u0026lt;- apprx(n = 200, p = .03, k = 6, R = 1000)\r# windows(6,5)\rboxplot(out$prob.pois, boxwex = 0.25, at = 1:1 - .25,\rcol = \u0026quot;green\u0026quot;,\rmain = expression(paste(\u0026quot;Approximating Binomial Probability: \u0026quot;, n==200, \u0026quot;, \u0026quot;, p==0.03, sep=\u0026quot;\u0026quot;)),\rylab = \u0026quot;Probablity\u0026quot;, ylim = c(out$trueval - 0.2, out$trueval + 0.25))\rboxplot(out$prob.zcc, boxwex = 0.25, at = 1:1 + 0, add = T,\rcol = \u0026quot;purple\u0026quot;)\rboxplot(out$prob.zncc, boxwex = 0.25, at = 1:1 + 0.25, add = T,\rcol = \u0026quot;orange\u0026quot; )\rabline(h = out$trueval, col = \u0026quot;red\u0026quot;, lty=2)\rlegend(\u0026quot;topleft\u0026quot;, c(\u0026quot;Poisson\u0026quot;, \u0026quot;Normal (with cc)\u0026quot;, \u0026quot;Normal (w/o cc)\u0026quot;), fill = c(\u0026quot;green\u0026quot;, \u0026quot;purple\u0026quot;, \u0026quot;orange\u0026quot;))\r\r","href":"/post/poisson-approximation-of-binomial-probabilities/","title":"Poisson approximation of binomial probabilities"},{"content":"","href":"/categories/analytics/","title":"Analytics"},{"content":"","href":"/authors/","title":"Authors"},{"content":"","href":"/categories/career/","title":"Career"},{"content":"","href":"/tags/career/","title":"Career"},{"content":"","href":"/categories/","title":"Categories"},{"content":"","href":"/tags/config/","title":"Configuration"},{"content":"","href":"/tags/data-science/","title":"Data Science"},{"content":"","href":"/authors/enayet/","title":"Enayet"},{"content":"","href":"/tags/git/","title":"Git"},{"content":"","href":"/authors/muniftanjim/","title":"Muniftanjim"},{"content":"","href":"/categories/notes/","title":"Notes"},{"content":"","href":"/tags/og/","title":"Opengraph"},{"content":"","href":"/page/","title":"Pages"},{"content":"","href":"/tags/plot/","title":"Plot"},{"content":"","href":"/tags/powerbi/","title":"PowerBI"},{"content":"","href":"/tags/python/","title":"Python"},{"content":"","href":"/categories/r/","title":"R"},{"content":"","href":"/tags/r/","title":"R"},{"content":"","href":"/tags/r-markdown/","title":"R Markdown"},{"content":"","href":"/tags/regression/","title":"Regression"},{"content":"","href":"/search/","title":"Search"},{"content":"","href":"/tags/statjobs/","title":"StatJobs"},{"content":"","href":"/tags/","title":"Tags"}]
